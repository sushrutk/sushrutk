<!DOCTYPE html>

<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="preconnect" href="https://fonts.googleapis.com"> 
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300&display=swap" rel="stylesheet">
<link rel="stylesheet" href="mystyle.css">
<title>Sushrut Karmalkar</title>
<style type="text/css">
body
{margin:20px auto;
max-width:800px;
line-height:1.6;
font-size:15px;
font-family: 'Roboto', sans-serif;;
color:#000;
padding:10px}

h1{font-size: 20px},
h2{font-size: 20px},
h3{line-height:1.2; font-size: 20px}
</style>
</head>

<h1> 
<p style="text-align:left;">
    <a href="index.html", style="color: #94618E"> Sushrut Karmalkar </a> 
    <span style="float:right;font-size: 15px">
        <a href="publications.html", style="color: #94618E">Publications</a>&nbsp&nbsp 
        <a href="#CV", style="color: #94618E">CV</a> &nbsp&nbsp 
  		<a href="#Contact", style="color: #94618E"> Contact</a>
    </span>
</p>
</h1>


<body>

</header><h2 style="font-size: 20px">Publications</h2>
<ol>
	<li><b>Approximation Schemes for ReLU Regression</b>
	<br>Ilias Diakonikolas, Surbhi Goel, <b> Sushrut Karmalkar, </b> Adam Klivans and Mahdi Soltanolkotabi
	<br><i>Conference on Learning Theory (COLT) 2020</i>
	<br> [<a href="http://arxiv.org/abs/2005.12844" style="color: #94618E">arxiv</a>]<br> </li>

	<li><b>Robustly Learning any Clusterable Mixture of Gaussians</b>
	<br>Ilias Diakonikolas, Samuel B. Hopkins, Daniel Kane and <b> Sushrut Karmalkar</b>
	<br><i>IEEE Symposium on Foundations of Computer Science (FOCS) 2020</i>
	<br> [<a href="https://arxiv.org/abs/2005.06417" style="color: #94618E">arxiv</a>]
	<br> [Note: Conference paper to be merged with <a href="https://arxiv.org/abs/2005.02970"> this</a> paper.] </br></li>  

	<li><b>On the Power of Compressed Sensing with Generative Models</b>
		<br>Akshay Kamath, <b> Sushrut Karmalkar </b> and Eric Price
		<br><i>International Conference on Machine Learning (ICML) 2020</i>
		<br> [<a href="https://arxiv.org/abs/1912.02938" style="color: #94618E">arxiv</a>]<br> </li>


	<li><b>Superpolynomial Lower Bounds for Learning One-Layer Neural Networks using Gradient Descent</b>
	<br>Surbhi Goel, Aravind Gollakota, Zhihan Jin, <b>Sushrut Karmalkar</b> and Adam Klivans
	<br><i>International Conference on Machine Learning (ICML) 2020</i> 
	<br>[<a href="https://arxiv.org/abs/2006.12011" style="color: #94618E">arxiv</a>]<br></li>

	<li><b>Time/Accuracy Tradeoffs for Learning a ReLU with respect to Gaussian Marginals</b>
            	<br> Surbhi Goel,  <b>Sushrut Karmalkar</b> and Adam Klivans
	 <br><i>Neural Information Processing Systems (NeurIPS) 2019</i> (Spotlight)
	<br> [<a href="https://arxiv.org/abs/1911.01462" style="color: #94618E">arxiv</a>]<br></li>

	<li><b>List decodeable linear regression</b>
            	<br> <b>Sushrut Karmalkar, </b> Adam Klivans and Pravesh Kothari
	 <br><i>Neural Information Processing Systems (NeurIPS) 2019</i> (Spotlight)
	<br> [<a href="https://arxiv.org/abs/1905.05679" style="color: #94618E">arxiv</a>]<br></li>

	<li><b>Outlier-Robust High-Dimensional Sparse Estimation via Iterative Filtering.</b>
            	<br> Ilias Diakonikolas, Daniel Kane, <b> Sushrut Karmalkar, </b> Eric Price and Alistair Stewart
	 <br><i>Neural Information Processing Systems (NeurIPS) 2019</i> 
	<br> [<a href="https://arxiv.org/abs/1911.08085" style="color: #94618E">arxiv</a>]<br></li>

	<li><b>Compressed Sensing with Adversarial Sparse Noise via L1 Regression.</b>
            	<br> <b> Sushrut Karmalkar</b> and Eric Price
	 <br><i> Symposium on Simplicity in Algorithms (SOSA) 2019</i>
	<br> [<a href="https://arxiv.org/abs/1809.08055" style="color: #94618E">arxiv</a>]<br></li>

	<li><b>Fourier Entropy-Influence Conjecture for Random Linear Threshold Functions</b>
            	<br> Sourav Chakraborty, <b> Sushrut Karmalkar</b>, Srijita Kundu, Satyanarayana V. Lokam and Nitin Saurabh
	 <br><i> Latin American Symposium on Theoretical Informatics (LATIN)  2018</i>
	<br> [<a href="https://arxiv.org/abs/1903.11635" style="color: #94618E">arxiv</a>]<br></li>

	<li><b>Robust Polynomial Regression up to the Information Theoretic Limit</b>
            	<br> Daniel Kane, <b> Sushrut Karmalkar</b> and Eric Price
	 <br><i>IEEE Symposium on Foundations of Computer Science (FOCS) 2017</i>
	<br> [<a href="https://arxiv.org/abs/1708.03257" style="color: #94618E">arxiv</a>]<br></li>
</ol>

